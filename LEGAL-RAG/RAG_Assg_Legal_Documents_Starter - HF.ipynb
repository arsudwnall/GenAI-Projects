{"cells":[{"cell_type":"markdown","id":"lf5lYawIw8tE","metadata":{"id":"lf5lYawIw8tE"},"source":["# **Extracting Information from Legal Documents Using RAG**"]},{"cell_type":"markdown","id":"NY1InIbkw80B","metadata":{"id":"NY1InIbkw80B"},"source":["## **Objective**"]},{"cell_type":"markdown","id":"3403a4b5","metadata":{"id":"3403a4b5"},"source":["The main objective of this assignment is to process and analyse a collection text files containing legal agreements (e.g., NDAs) to prepare them for implementing a **Retrieval-Augmented Generation (RAG)** system. This involves:\n","\n","* Understand the Cleaned Data : Gain a comprehensive understanding of the structure, content, and context of the cleaned dataset.\n","* Perform Exploratory Analysis : Conduct bivariate and multivariate analyses to uncover relationships and trends within the cleaned data.\n","* Create Visualisations : Develop meaningful visualisations to support the analysis and make findings interpretable.\n","* Derive Insights and Conclusions : Extract valuable insights from the cleaned data and provide clear, actionable conclusions.\n","* Document the Process : Provide a detailed description of the data, its attributes, and the steps taken during the analysis for reproducibility and clarity.\n","\n","The ultimate goal is to transform the raw text data into a clean, structured, and analysable format that can be effectively used to build and train a RAG system for tasks like information retrieval, question-answering, and knowledge extraction related to legal agreements."]},{"cell_type":"markdown","id":"3TTEcbb5hIM-","metadata":{"id":"3TTEcbb5hIM-"},"source":["### **Business Value**  \n"]},{"cell_type":"markdown","id":"ZsfkEL2CgljF","metadata":{"id":"ZsfkEL2CgljF"},"source":["The project aims to leverage RAG to enhance legal document processing for businesses, law firms, and regulatory bodies. The key business objectives include:\n","\n","* Faster Legal Research: <br> Reduce the time lawyers and compliance officers spend searching for relevant case laws, precedents, statutes, or contract clauses.\n","* Improved Contract Analysis: <br> Automatically extract key terms, obligations, and risks from lengthy contracts.\n","* Regulatory Compliance Monitoring: <br> Help businesses stay updated with legal and regulatory changes by retrieving relevant legal updates.\n","* Enhanced Decision-Making: <br> Provide accurate and context-aware legal insights to assist in risk assessment and legal strategy.\n","\n","\n","**Use Cases**\n","* Legal Chatbots\n","* Contract Review Automation\n","* Tracking Regulatory Changes and Compliance Monitoring\n","* Case Law Analysis of past judgments\n","* Due Diligence & Risk Assessment"]},{"cell_type":"markdown","id":"rDp_EWxVOhUu","metadata":{"id":"rDp_EWxVOhUu"},"source":["## **1. Data Loading, Preparation and Analysis** <font color=red> [20 marks] </font><br>"]},{"cell_type":"markdown","id":"JZGTCfyUxalZ","metadata":{"id":"JZGTCfyUxalZ"},"source":["### **1.1 Data Understanding**"]},{"cell_type":"markdown","id":"ok6sSYNAiG8V","metadata":{"id":"ok6sSYNAiG8V"},"source":["The dataset contains legal documents and contracts collected from various sources. The documents are present as text files (`.txt`) in the *corpus* folder.\n","\n","There are four types of documents in the *courpus* folder, divided into four subfolders.\n","- `contractnli`: contains various non-disclosure and confidentiality agreements\n","- `cuad`: contains contracts with annotated legal clauses\n","- `maud`: contains various merger/acquisition contracts and agreements\n","- `privacy_qa`: a question-answering dataset containing privacy policies\n","\n","The dataset also contains evaluation files in JSON format in the *benchmark* folder. The files contain the questions and their answers, along with sources. For each of the above four folders, there is a `json` file: `contractnli.json`, `cuad.json`, `maud.json` `privacy_qa.json`. The file structure is as follows:\n","\n","```\n","{\n","    \"tests\": [\n","        {\n","            \"query\": <question1>,\n","            \"snippets\": [{\n","                    \"file_path\": <source_file1>,\n","                    \"span\": [ begin_position, end_position ],\n","                    \"answer\": <relevant answer to the question 1>\n","                },\n","                {\n","                    \"file_path\": <source_file2>,\n","                    \"span\": [ begin_position, end_position ],\n","                    \"answer\": <relevant answer to the question 2>\n","                }, ....\n","            ]\n","        },\n","        {\n","            \"query\": <question2>,\n","            \"snippets\": [{<answer context for que 2>}]\n","        },\n","        ... <more queries>\n","    ]\n","}\n","```"]},{"cell_type":"markdown","id":"S7Ac8VxvjWnw","metadata":{"id":"S7Ac8VxvjWnw"},"source":["### **1.2 Load and Preprocess the data** <font color=red> [5 marks] </font><br>"]},{"cell_type":"markdown","id":"gJ8fA4Nh3fHg","metadata":{"id":"gJ8fA4Nh3fHg"},"source":["#### Loading libraries"]},{"cell_type":"code","execution_count":1,"id":"BqyFHhSn48tC","metadata":{"id":"BqyFHhSn48tC"},"outputs":[],"source":["## The following libraries might be useful\n","# !pip install -q langchain-openai\n","# !pip install -U -q langchain-community\n","# !pip install -U -q langchain-chroma\n","# !pip install -U -q datasets\n","# !pip install -U -q ragas\n","# !pip install -U -q rouge_score"]},{"cell_type":"code","execution_count":2,"id":"Qpn-qbhAi58F","metadata":{"id":"Qpn-qbhAi58F"},"outputs":[],"source":["# Import essential libraries\n","#from langchain_openai import ChatOpenAI\n","from langchain_groq import ChatGroq\n","from langchain_core.prompts import PromptTemplate\n","from langchain_community.document_loaders import TextLoader\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_community.vectorstores import Chroma\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","import os\n","from langchain.vectorstores import FAISS\n","#from langchain.embeddings import OpenAIEmbeddings\n","\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","from langchain_core.runnables import RunnableMap\n","from langchain.embeddings.cache import CacheBackedEmbeddings\n","from langchain.prompts import ChatPromptTemplate\n","from langchain.schema.runnable import RunnablePassthrough\n","from langchain.storage import LocalFileStore"]},{"cell_type":"markdown","id":"zOMf-tfIiOlp","metadata":{"id":"zOMf-tfIiOlp"},"source":["#### **1.2.1** <font color=red> [3 marks] </font>\n","Load all `.txt` files from the folders."]},{"cell_type":"markdown","id":"f2ea36ba","metadata":{"id":"f2ea36ba"},"source":["You can utilise document loaders from the options provided by the LangChain community.\n","\n","Optionally, you can also read the files manually, while ensuring proper handling of encoding issues (e.g., utf-8, latin1). In such case, also store the file content along with metadata (e.g., file name, directory path) for traceability."]},{"cell_type":"code","execution_count":4,"id":"I9rTY8DWx2Wj","metadata":{"id":"I9rTY8DWx2Wj"},"outputs":[],"source":["# Load the files as documents\n","\n","docs = []  # List to hold the documents\n","\n","for folder in ['contractnli', 'cuad', 'maud', 'privacy_qa']:\n","    folder_path = f\"./rag_legal/corpus/{folder}\"\n","    for filename in os.listdir(folder_path):\n","        if filename.endswith(\".txt\"):\n","            loader = TextLoader(os.path.join(folder_path, filename), encoding='utf-8')\n","            document = loader.load()[0]\n","            document.metadata[\"source\"] = filename\n","            document.metadata[\"category\"] = folder\n","            docs.append(document)\n"]},{"cell_type":"code","execution_count":5,"id":"e4af70fb","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Confidentiality Agreement\n","This Confidentiality Agreement (the “Agreement”) has been prepared on behalf of bpost SA/NV, a limited liability company of public law, and the City of Brussels (the “Sellers”) with regard to the divestment of their respective surface areas (the “Sale”) in the Munt Center/Centre Monnaie (the “Munt”).\n","In consideration of the Sellers agreeing to disclose confidential information (the “Confidential Information”) to the Candidate (“Disclosee”), the latter hereby undertakes \n"]}],"source":["# check the first document in doc\n","print(docs[1].page_content[:500])  # Display first 500 characters"]},{"cell_type":"markdown","id":"K4HYLoUjwmMs","metadata":{"id":"K4HYLoUjwmMs"},"source":["#### **1.2.2** <font color=red> [2 marks] </font>\n","Preprocess the text data to remove noise and prepare it for analysis."]},{"cell_type":"markdown","id":"e9793fdf","metadata":{"id":"e9793fdf"},"source":["# Clean and preprocess the data\n","Remove special characters, extra whitespace, and irrelevant content such as email and telephone contact info.\n","Normalise text (e.g., convert to lowercase, remove stop words).\n","Handle missing or corrupted data by logging errors and skipping problematic files."]},{"cell_type":"code","execution_count":6,"id":"5f111e47","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import re\n","import nltk\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":7,"id":"1ec87e69","metadata":{"id":"1ec87e69"},"outputs":[],"source":["\n","# Define your text cleaning function to remove special characters, emails, and phone numbers and \n","# to normalize text to lowercase and remove stop words\n","\n","from nltk.corpus import stopwords\n","\n","# Load English stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","def clean_text(text: str) -> str:\n","    # Remove emails and phone numbers\n","    text = re.sub(r'\\S+@\\S+', '', text)\n","    text = re.sub(r'\\+?\\d[\\d\\-\\(\\) ]{7,}\\d', '', text)\n","    \n","    # Remove special characters except common punctuation\n","    text = re.sub(r'[^\\w\\s\\.,;:\\-\\(\\)]', '', text)\n","    \n","    # Lowercase and normalize spaces\n","    text = text.lower()\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","\n","    # Remove stop words using a library or predefined list\n","  \n","    words = text.split()\n","    filtered_words = [word for word in words if word not in stop_words]\n","    text = ' '.join(filtered_words)\n","\n","    return text\n"]},{"cell_type":"code","execution_count":8,"id":"434eba9e","metadata":{},"outputs":[],"source":["# Apply clean_text to each document in `docs`\n","for doc in docs:\n","    raw_text = doc.page_content\n","    cleaned_text = clean_text(raw_text)\n","    doc.page_content = cleaned_text"]},{"cell_type":"markdown","id":"b9e90470","metadata":{"id":"b9e90470"},"source":["### **1.3 Exploratory Data Analysis** <font color=red> [10 marks] </font><br>"]},{"cell_type":"markdown","id":"Nd1K4yhIzyPp","metadata":{"id":"Nd1K4yhIzyPp"},"source":["#### **1.3.1** <font color=red> [1 marks] </font>\n","Calculate the average, maximum and minimum document length."]},{"cell_type":"code","execution_count":9,"id":"tQT1UIcOHSp9","metadata":{"id":"tQT1UIcOHSp9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average document length: 9276.018624641834\n","Maximum document length: 87912\n","Minimum document length: 148\n"]}],"source":["# Calculate the average, maximum and minimum document length.\n","\n","doc_lengths = [len(doc.page_content.split()) for doc in docs]\n","\n","average_length = sum(doc_lengths) / len(doc_lengths)\n","max_length = max(doc_lengths)\n","min_length = min(doc_lengths)\n","print(f\"Average document length: {average_length}\")\n","print(f\"Maximum document length: {max_length}\")\n","print(f\"Minimum document length: {min_length}\")\n","\n"]},{"cell_type":"markdown","id":"18xQu__O0wLv","metadata":{"id":"18xQu__O0wLv"},"source":["#### **1.3.2** <font color=red> [4 marks] </font>\n","Analyse the frequency of occurrence of words and find the most and least occurring words."]},{"cell_type":"markdown","id":"IQ_i5YfFH2dg","metadata":{"id":"IQ_i5YfFH2dg"},"source":["Find the 20 most common and least common words in the text. Ignore stop words such as articles and prepositions."]},{"cell_type":"code","execution_count":10,"id":"Q8eiDTy2Ic8z","metadata":{"id":"Q8eiDTy2Ic8z"},"outputs":[{"name":"stdout","output_type":"stream","text":["Most common words:\n","company: 131502\n","shall: 104461\n","section: 74793\n","agreement: 64825\n","parent: 48768\n","party: 41770\n","material: 33344\n","date: 29172\n","merger: 28577\n","respect: 28337\n"]}],"source":["# Find frequency of occurence of words\n","\n","from collections import Counter\n","word_counts = Counter()\n","for doc in docs:\n","    words = doc.page_content.split()\n","    word_counts.update(words)\n","# Display the 10 most common words\n","most_common_words = word_counts.most_common(10)\n","print(\"Most common words:\")\n","for word, count in most_common_words:\n","    print(f\"{word}: {count}\")\n","\n"]},{"cell_type":"markdown","id":"xlF55RNjz9pQ","metadata":{"id":"xlF55RNjz9pQ"},"source":["#### **1.3.3** <font color=red> [4 marks] </font>\n","Analyse the similarity of different documents to each other based on TF-IDF vectors."]},{"cell_type":"markdown","id":"jciCNMelOGPJ","metadata":{"id":"jciCNMelOGPJ"},"source":["Transform some documents to TF-IDF vectors and calculate their similarity matrix using a suitable distance function. If contracts contain duplicate or highly similar clauses, similarity calculation can help detect them.\n","\n","Identify for the first 10 documents and then for 10 random documents. What do you observe?"]},{"cell_type":"code","execution_count":11,"id":"b4d42269","metadata":{},"outputs":[],"source":["\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity"]},{"cell_type":"code","execution_count":12,"id":"M-_SrvDcMnKi","metadata":{"id":"M-_SrvDcMnKi"},"outputs":[{"name":"stdout","output_type":"stream","text":["Similarity scores for the first 3 documents:\n","Document 1: [1.         0.26971714 0.60257809 0.48480418 0.50785621 0.55648004\n"," 0.57321136 0.70233178 0.43619587 0.54373699]\n","Document 2: [0.26971714 1.         0.27438624 0.20114994 0.21927664 0.23430334\n"," 0.22732688 0.25520159 0.24895088 0.24935309]\n","Document 3: [0.60257809 0.27438624 1.         0.63127777 0.68752384 0.70919393\n"," 0.71713148 0.75051869 0.40434704 0.6816036 ]\n"]}],"source":["# Transform the page contents of documents\n","# Take first 10 documents\n","subset_docs = docs[:10]\n","\n","# extract the text content from subset_docs\n","documents_text = [doc.page_content for doc in subset_docs]\n","\n","# Vectorize the documents using TF-IDF\n","vectorizer = TfidfVectorizer()\n","tfidf_matrix = vectorizer.fit_transform(documents_text)\n","\n","# Compute similarity scores\n","similarity_matrix = cosine_similarity(tfidf_matrix)\n","\n","# Display the 3 similarity scores\n","print(\"Similarity scores for the first 3 documents:\")\n","for i in range(3):\n","    print(f\"Document {i+1}: {similarity_matrix[i]}\")\n"]},{"cell_type":"code","execution_count":13,"id":"pd99eXtnK2DU","metadata":{"id":"pd99eXtnK2DU"},"outputs":[{"name":"stdout","output_type":"stream","text":["[11, 89, 59, 8, 21, 1, 54, 87, 33, 11]\n"]}],"source":["import random\n","# create a list of 10 random integers\n","random_integers = [random.randint(1, 100) for _ in range(10)]\n","print(random_integers)\n"]},{"cell_type":"code","execution_count":14,"id":"t31ngfZTJimS","metadata":{"id":"t31ngfZTJimS"},"outputs":[{"name":"stdout","output_type":"stream","text":["Similarity scores for the random documents:\n","Document 1: [1.         0.55341428 0.55653663 0.30745814 0.66558479 0.21229147\n"," 0.28271718 0.60035389 0.58118459 1.        ]\n","Document 2: [0.55341428 1.         0.53174891 0.30262517 0.581909   0.19882509\n"," 0.28412904 0.54992018 0.54072716 0.55341428]\n","Document 3: [0.55653663 0.53174891 1.         0.34231109 0.60190613 0.2447983\n"," 0.31052082 0.51881438 0.59915872 0.55653663]\n","Document 4: [0.30745814 0.30262517 0.34231109 1.         0.3338635  0.23549735\n"," 0.25438761 0.25840538 0.36332278 0.30745814]\n","Document 5: [0.66558479 0.581909   0.60190613 0.3338635  1.         0.23802778\n"," 0.30739705 0.62880051 0.68212685 0.66558479]\n","Document 6: [0.21229147 0.19882509 0.2447983  0.23549735 0.23802778 1.\n"," 0.17862692 0.17808256 0.27206287 0.21229147]\n","Document 7: [0.28271718 0.28412904 0.31052082 0.25438761 0.30739705 0.17862692\n"," 1.         0.26019682 0.32340234 0.28271718]\n","Document 8: [0.60035389 0.54992018 0.51881438 0.25840538 0.62880051 0.17808256\n"," 0.26019682 1.         0.56469699 0.60035389]\n","Document 9: [0.58118459 0.54072716 0.59915872 0.36332278 0.68212685 0.27206287\n"," 0.32340234 0.56469699 1.         0.58118459]\n","Document 10: [1.         0.55341428 0.55653663 0.30745814 0.66558479 0.21229147\n"," 0.28271718 0.60035389 0.58118459 1.        ]\n"]}],"source":["# Compute similarity scores for 10 random documents\n","# use the random integers to select documents\n","random_docs = [docs[i] for i in random_integers if i < len(docs)]\n","\n","# extract the text content from random_docs\n","random_documents_text = [doc.page_content for doc in random_docs]\n","\n","# Vectorize the random documents using TF-IDF\n","random_tfidf_matrix = vectorizer.fit_transform(random_documents_text)\n","# Compute similarity scores for the random documents\n","random_similarity_matrix = cosine_similarity(random_tfidf_matrix)\n","# Display the similarity scores for the random documents\n","print(\"Similarity scores for the random documents:\")\n","for i in range(len(random_docs)):\n","    print(f\"Document {i+1}: {random_similarity_matrix[i]}\")\n","\n"]},{"cell_type":"markdown","id":"3cfd0f53","metadata":{"id":"3cfd0f53"},"source":["### **1.4 Document Creation and Chunking** <font color=red> [5 marks] </font><br>"]},{"cell_type":"markdown","id":"pCw3NzcE3waS","metadata":{"id":"pCw3NzcE3waS"},"source":["#### **1.4.1** <font color=red> [5 marks] </font>\n","Perform appropriate steps to split the text into chunks."]},{"cell_type":"code","execution_count":15,"id":"TjZ6yf9r2p1F","metadata":{"id":"TjZ6yf9r2p1F"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total chunks created: 43241\n"]}],"source":["# Process files and generate chunks\n","\n","text_splitter = RecursiveCharacterTextSplitter(\n","    chunk_size=1500,\n","    chunk_overlap=100,\n","    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",")\n","\n","# to hold the chunks \n","chunks = []\n","\n","for doc in docs:\n","    doc_chunks = text_splitter.split_documents([doc])\n","    # split_documents  Preserves metadata (e.g., file name, source, category) over split text \n","    # Returns a list of Document objects — ideal for use in vector stores and RAG pipelines.\n","    chunks.extend(doc_chunks)\n","\n","\n","#check how many chunks were created\n","print(f\"Total chunks created: {len(chunks)}\")  \n","\n"]},{"cell_type":"code","execution_count":16,"id":"1bbbbffb","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["First chunk content:\n","mutual non-disclosure agreement subject matter: effective date agreement: period , 2017 exchange information: , 2017 period confidentiality: agreement made effective date agreement noted above, parties. background: i. parties desire discussions relating subject matter purposes evaluating possible bu\n"]}],"source":["# print first part of a chunk for checking\n","print(\"First chunk content:\")\n","print(chunks[0].page_content[:300])  "]},{"cell_type":"markdown","id":"LeAeTqpZ-DYw","metadata":{"id":"LeAeTqpZ-DYw"},"source":["## **2. Vector Database and RAG Chain Creation** <font color=red> [15 marks] </font><br>"]},{"cell_type":"markdown","id":"YoH_Ac6K6aQZ","metadata":{"id":"YoH_Ac6K6aQZ"},"source":["### **2.1 Vector Embedding and Vector Database Creation** <font color=red> [7 marks] </font><br>"]},{"cell_type":"markdown","id":"bBfj5ycC59lU","metadata":{"id":"bBfj5ycC59lU"},"source":["#### **2.1.1** <font color=red> [2 marks] </font>\n","Initialise an embedding function for loading the embeddings into the vector database."]},{"cell_type":"markdown","id":"v-QeR5N_7jiw","metadata":{"id":"v-QeR5N_7jiw"},"source":["Initialise a function to transform the text to vectors using OPENAI Embeddings module. You can also use this function to transform during vector DB creation itself."]},{"cell_type":"code","execution_count":17,"id":"611bc8a5","metadata":{},"outputs":[],"source":["import os\n","from dotenv import load_dotenv\n","\n","# Load from .env file\n","load_dotenv()\n","\n","# Access the key\n","hf_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"]},{"cell_type":"code","execution_count":20,"id":"bd0efb63","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["No sentence-transformers model found with name yuriyvnv/legal-bge-m3. Creating a new one with mean pooling.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"11ee6af7231e42529944aaac49df5630","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/757 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c475ce432eb241d0a4f59994bbcca144","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18ee23147a9e4c18b595bab6ff7a875c","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2e1ef6a638b84e9e9cab05f299cdd40c","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c31e0eb488f14773993fea56e49c27ce","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[ 0.01675033  0.04724129 -0.03916406 ... -0.00577066 -0.0168285\n","  0.00470762]\n"]}],"source":["from sentence_transformers import SentenceTransformer\n","\n","model = SentenceTransformer(\"yuriyvnv/legal-bge-m3\")\n","print(model.encode(\"Example sentence\", normalize_embeddings=True))"]},{"cell_type":"code","execution_count":19,"id":"2079c2cf","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["No sentence-transformers model found with name nlpaueb/legal-bert-base-uncased. Creating a new one with mean pooling.\n"]}],"source":["from transformers import AutoTokenizer, AutoModel\n","import torch\n","from langchain_community.embeddings import HuggingFaceEmbeddin\n","from sentence_transformers import SentenceTransformer, util\n","\n","#model = SentenceTransformer('yuriyvnv/legal-bge-m3')\n","\n","# Create embeddings instance (CORRECT WAY)\n","embedding_model = HuggingFaceEmbeddings(\n","    model_name=\"yuriyvnv/legal-bge-m3\",  # Still need to specify the name\n","    model_kwargs={\"device\": \"cpu\"},\n","    encode_kwargs={\"normalize_embeddings\": True}\n",")\n","\n","# Local cache store (saves embedding bytes keyed by hash)\n","store = LocalFileStore(\"./embedding_cache\")\n","\n","cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n","    embedding_model ,\n","    store,\n","    namespace=embedding_model.model_name  # avoids collisions with different models\n",")\n"]},{"cell_type":"markdown","id":"WTkTIerj5-KI","metadata":{"id":"WTkTIerj5-KI"},"source":["#### **2.1.2** <font color=red> [5 marks] </font>\n","Load the embeddings to a vector database."]},{"cell_type":"markdown","id":"o6rEbd7477R8","metadata":{"id":"o6rEbd7477R8"},"source":["Create a directory for vector database and enter embedding data to the vector DB."]},{"cell_type":"code","execution_count":20,"id":"IaqfjQJf2v8Y","metadata":{"id":"IaqfjQJf2v8Y"},"outputs":[{"ename":"NameError","evalue":"name 'cached_embedder' is not defined","output_type":"error","traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Add Chunks to vector DB\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m vectorstore = FAISS.from_documents(chunks, \u001b[43mcached_embedder\u001b[49m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Save the vectorstore to disk\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#vectorstore.save_local(\"rag_legal_vectorstore_Groq\")\u001b[39;00m\n\u001b[32m      6\u001b[39m \n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# to load the vectorstore from disk\u001b[39;00m\n\u001b[32m      8\u001b[39m vectorstore = FAISS.load_local(\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrag_legal_vectorstore_HF\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     10\u001b[39m     embedding_model,\n\u001b[32m     11\u001b[39m     allow_dangerous_deserialization=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     12\u001b[39m )\n","\u001b[31mNameError\u001b[39m: name 'cached_embedder' is not defined"]}],"source":["# Add Chunks to vector DB\n","vectorstore = FAISS.from_documents(chunks, cached_embedder)\n","\n","# Save the vectorstore to disk\n","vectorstore.save_local(\"rag_legal_vectorstore_HF\")\n","\n","# to load the vectorstore from disk\n","vectorstore = FAISS.load_local(\n","    \"rag_legal_vectorstore_HF\", \n","    embedding_model,\n","    allow_dangerous_deserialization=True\n",")\n"]},{"cell_type":"markdown","id":"978619ac","metadata":{"id":"978619ac"},"source":["### **2.2 Create RAG Chain** <font color=red> [8 marks] </font><br>"]},{"cell_type":"markdown","id":"Rczna1Xy_1bq","metadata":{"id":"Rczna1Xy_1bq"},"source":["#### **2.2.1** <font color=red> [5 marks] </font>\n","Create a RAG chain."]},{"cell_type":"code","execution_count":62,"id":"f6730b90","metadata":{},"outputs":[],"source":["# create a retriever from the vectorstore for top 3 results\n","retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"]},{"cell_type":"code","execution_count":63,"id":"6097169c","metadata":{},"outputs":[],"source":["llm = ChatGroq(\n","    temperature=0,\n","    model_name=\"llama3-70b-8192\"\n",")"]},{"cell_type":"code","execution_count":136,"id":"b826f2d2","metadata":{},"outputs":[],"source":["prompt = ChatPromptTemplate.from_template(\"\"\"\n","You are a legal assistant AI. Use the following context to answer the question concisely.\n","\n","Context:\n","{context}\n","\n","Question:\n","{question}\n","\"\"\")"]},{"cell_type":"code","execution_count":137,"id":"sEzxYN93Ygju","metadata":{"id":"sEzxYN93Ygju"},"outputs":[],"source":["# Create a RAG chain\n","rag_chain = (\n","    {\"context\": retriever, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | llm\n",")"]},{"cell_type":"markdown","id":"6PkgzeTIElfy","metadata":{"id":"6PkgzeTIElfy"},"source":["#### **2.2.2** <font color=red> [3 marks] </font>\n","Create a function to generate answer for asked questions."]},{"cell_type":"markdown","id":"W8AMtr94FZxR","metadata":{"id":"W8AMtr94FZxR"},"source":["Use the RAG chain to generate answer for a question and provide source documents"]},{"cell_type":"code","execution_count":138,"id":"b9TQdz5uFzlr","metadata":{"id":"b9TQdz5uFzlr"},"outputs":[],"source":["# Create a function for question answering\n","def ask_legal_question(rag_chain, question: str) -> str:\n","    try:\n","        response = rag_chain.invoke(question)\n","        return response.content\n","    except Exception as e:\n","        # retunr the error message\n","        return f\"Error: {str(e)}\""]},{"cell_type":"markdown","id":"ce774c0c","metadata":{},"source":["Lnahchain Embeds the question using the same embedding model used to build the vectorstore (e.g., OpenAIEmbeddings)."]},{"cell_type":"code","execution_count":139,"id":"pSgcP19iYgjv","metadata":{"id":"pSgcP19iYgjv"},"outputs":[{"name":"stdout","output_type":"stream","text":["Answer: Yes, the document indicates that the Agreement does not grant the Receiving Party any rights to the Confidential Information. Specifically, it states: \"notwithstanding mentors right assess rate ideas participants, mentor shall use confidential information hisher third parties purposes shall file intellectual property right protection confidential information parts it.\" This implies that the Receiving Party (mentor) does not acquire any rights to the Confidential Information, and any use or protection of such information is limited to the purposes specified in the Agreement.\n"]}],"source":["# Example question\n","question =\"Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document indicate that the Agreement does not grant the Receiving Party any rights to the Confidential Information?\"\n","answer = ask_legal_question(rag_chain, question)\n","print(\"Answer:\", answer)"]},{"cell_type":"markdown","id":"fMmX8OrcN05D","metadata":{"id":"fMmX8OrcN05D"},"source":["## **3. RAG Evaluation** <font color=red> [10 marks] </font><br>"]},{"cell_type":"markdown","id":"b1ddfed9","metadata":{"id":"b1ddfed9"},"source":["### **3.1 Evaluation and Inference** <font color=red> [10 marks] </font><br>"]},{"cell_type":"markdown","id":"Z9xy_GduS9Yk","metadata":{"id":"Z9xy_GduS9Yk"},"source":["#### **3.1.1** <font color=red> [2 marks] </font>\n","Extract all the questions and all the answers/ground truths from the benchmark files."]},{"cell_type":"markdown","id":"V397RqkRfjSP","metadata":{"id":"V397RqkRfjSP"},"source":["Create a questions set and an answers set containing all the questions and answers from the benchmark files to run evaluations."]},{"cell_type":"code","execution_count":140,"id":"25feec7c","metadata":{},"outputs":[],"source":["import json"]},{"cell_type":"code","execution_count":141,"id":"bF_KZXb1c-G5","metadata":{"id":"bF_KZXb1c-G5"},"outputs":[],"source":["# Create a question set by taking all the questions from the benchmark data\n","# Also create a ground truth/answer set\n","def load_benchmark_data(benchmark_dir: str):\n","    benchmark_data = []\n","    for file_name in ['contractnli.json', 'cuad.json', 'maud.json', 'privacy_qa.json']:\n","        file_path = os.path.join(benchmark_dir, file_name)\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            data = json.load(f)\n","            if 'tests' in data:\n","                for item in data['tests']:\n","                    question = item.get('query', '')\n","                    if item.get(\"snippets\"):\n","                        for snippet in item[\"snippets\"]:\n","                            answer = snippet.get('answer', '')\n","                            benchmark_data.append({\n","                                \"question\": question,\n","                                \"answer\": answer.strip()\n","                            })\n","    return benchmark_data"]},{"cell_type":"code","execution_count":142,"id":"e7d1a546","metadata":{},"outputs":[],"source":["benchmark_data = load_benchmark_data(\"./rag_legal/benchmarks\")"]},{"cell_type":"code","execution_count":143,"id":"0131b367","metadata":{},"outputs":[{"data":{"text/plain":["[{'question': 'Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document indicate that the Agreement does not grant the Receiving Party any rights to the Confidential Information?',\n","  'answer': 'Any and all proprietary rights, including but not limited to rights to and in inventions, patent rights, utility models, copyrights, trademarks and trade secrets, in and to any Confidential Information shall be and remain with the Participants respectively, and Mentor shall not have any right, license, title or interest in or to any Confidential Information, except the limited right to review, assess and help develop such Confidential Information in connection with the Copernicus Accelerator 2017.'},\n"," {'question': 'Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document state that Confidential Information shall only include technical information?',\n","  'answer': '“Confidential Information” means any Idea disclosed to Mentor, all data and information, know-how, business concepts, software, procedures, products, services, development projects, and programmes contained in such Idea and/or its description and any conclusions.'},\n"," {'question': 'Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document mention that some obligations of the Agreement may survive the termination of the Agreement?',\n","  'answer': 'Notwithstanding the termination of this Agreement, any Confidential Information must be kept confidential for as long as such Confidential Information is not publicly known unless it becomes part of the public domain through no wrongful act of Mentor.'},\n"," {'question': 'Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document permit the Receiving Party to retain some Confidential Information even after its return or destruction?',\n","  'answer': 'At Organiser’s first request, Mentor shall:'},\n"," {'question': 'Consider the Non-Disclosure Agreement between CopAcc and ToP Mentors; Does the document permit the Receiving Party to retain some Confidential Information even after its return or destruction?',\n","  'answer': '(d) erase and/or destroy any Confidential Information contained in computer memory or data storage apparatus of, under control of or used by Mentor;\\n(e) remove the Confidential Information from any software or data base of, under control of/or used by Mentor that incorporates or uses the Confidential Information in whole or in part; and'}]"]},"execution_count":143,"metadata":{},"output_type":"execute_result"}],"source":["benchmark_data[0:5]  # Display the first 5 questions and answers from the benchmark data"]},{"cell_type":"markdown","id":"81VscxuGTHhC","metadata":{"id":"81VscxuGTHhC"},"source":["#### **3.1.2** <font color=red> [5 marks] </font>\n","Create a function to evaluate the generated answers."]},{"cell_type":"markdown","id":"qIPUg1dKTPGb","metadata":{"id":"qIPUg1dKTPGb"},"source":["Evaluate the responses on *Rouge*, *Ragas* and *Bleu* scores."]},{"cell_type":"code","execution_count":144,"id":"b17f5bcc","metadata":{},"outputs":[],"source":["def evaluate_rag_chain(rag_chain, benchmark_data):\n","    predictions = []\n","    for item in benchmark_data:\n","        question = item[\"question\"]\n","        true_answer = item[\"answer\"]\n","        try:\n","            response = rag_chain.invoke(question)\n","            predicted_answer = response.content if hasattr(response, \"content\") else str(response)\n","        except Exception as e:\n","            predicted_answer = f\"Error: {e}\"\n","\n","        predictions.append({\n","            \"question\": question,\n","            \"true_answer\": true_answer,\n","            \"predicted_answer\": predicted_answer\n","        })\n","\n","    return predictions"]},{"cell_type":"code","execution_count":145,"id":"1a8ec409","metadata":{},"outputs":[],"source":["from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"]},{"cell_type":"code","execution_count":146,"id":"RuoBJS5_PKmX","metadata":{"id":"RuoBJS5_PKmX"},"outputs":[],"source":["# Function to evaluate the RAG pipeline\n","def compute_scores(predictions):\n","    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    results = []\n","\n","    for item in predictions:\n","        ref = item[\"true_answer\"]\n","        hyp = item[\"predicted_answer\"]\n","\n","        # ROUGE\n","        rouge = scorer.score(ref, hyp)[\"rougeL\"].fmeasure\n","\n","        # BLEU\n","        smoothie = SmoothingFunction().method4\n","        bleu = sentence_bleu([ref.split()], hyp.split(), smoothing_function=smoothie)\n","\n","        results.append({\n","            \"question\": item[\"question\"],\n","            \"true_answer\": ref,\n","            \"predicted_answer\": hyp,\n","            \"rougeL\": round(rouge, 4),\n","            \"bleu\": round(bleu, 4)\n","        })\n","\n","    return results"]},{"cell_type":"code","execution_count":147,"id":"4fa1efb9","metadata":{},"outputs":[],"source":["benchmark_data = load_benchmark_data(\"./rag_legal/benchmarks\")"]},{"cell_type":"markdown","id":"Omeb5vFSTbS0","metadata":{"id":"Omeb5vFSTbS0"},"source":["#### **3.1.3** <font color=red> [3 marks] </font>\n","Draw inferences by evaluating answers to all questions."]},{"cell_type":"markdown","id":"ei2qIN71Tirg","metadata":{"id":"ei2qIN71Tirg"},"source":["To save time and computing power, you can just run the evaluation on first 100 questions."]},{"cell_type":"code","execution_count":148,"id":"f4f1f24a","metadata":{"id":"f4f1f24a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Average ROUGE-L: 0.2221\n","Average BLEU: 0.0488\n"]}],"source":["# Evaluate the RAG pipeline\n","\n","predictions = evaluate_rag_chain(rag_chain, benchmark_data[:20])\n","results = compute_scores(predictions)\n","\n","# print the average scores\n","average_rougeL = sum(r[\"rougeL\"] for r in results) / len(results)\n","average_bleu = sum(r[\"bleu\"] for r in results) / len(results)\n","print(f\"Average ROUGE-L: {average_rougeL:.4f}\")\n","print(f\"Average BLEU: {average_bleu:.4f}\")"]},{"cell_type":"markdown","id":"a34eef05","metadata":{},"source":["Average ROUGE-L: 0.2221\n","Average BLEU: 0.0488"]},{"cell_type":"markdown","id":"gonMO9wNE5dt","metadata":{"id":"gonMO9wNE5dt"},"source":["## **4. Conclusion** <font color=red> [5 marks] </font><br>"]},{"cell_type":"markdown","id":"ySHPR29rE5du","metadata":{"id":"ySHPR29rE5du"},"source":["### **4.1 Conclusions and insights** <font color=red> [5 marks] </font><br>"]},{"cell_type":"markdown","id":"KoVsmcV0E5du","metadata":{"id":"KoVsmcV0E5du"},"source":["#### **4.1.1** <font color=red> [5 marks] </font>\n","Conclude with the results here. Include the insights gained about the data, model pipeline, the RAG process and the results obtained."]},{"cell_type":"markdown","id":"49c80758","metadata":{},"source":["We did the embedding and the open sourced model for 100 documents we received \n","ROUGE-L: Measures the overlap between the predicted and reference (true) answers, especially focusing on sequence — it checks how well the order of words is preserved.\n","ROUGE-L = 0.2564:\n","This shows some overlap in sequences between the generated and actual answers.\n","Not great, but not zero — there is semantic or lexical similarity.\n","In real-world open-ended QA or summarization, a ROUGE-L around 0.2–0.3 is considered moderate.\n","\n","BLEU checks how many n-grams (1-gram, 2-gram, etc.) in the prediction match those in the reference. It emphasizes precision, not recall. That means it cares how many of the words/phrases in the predicted answer are correct (not if you caught everything).\n","BLEU scores also range from 0 to 1 \n","BLEU = 0.02:\n","Quite low — indicating that very few exact n-gram matches exist.\n","Common in open-ended, abstractive answers where phrasing can vary a lot.\n","BLEU works best for exact matches or short answer generation; less effective in legal QA or paraphrased answers."]},{"cell_type":"markdown","id":"37b698d0","metadata":{},"source":["I tried changign prompts to make it specific search but that lead to lower rogue as so answers are generic answers \n"]}],"metadata":{"colab":{"collapsed_sections":["LeAeTqpZ-DYw","fMmX8OrcN05D","gonMO9wNE5dt"],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":5}
